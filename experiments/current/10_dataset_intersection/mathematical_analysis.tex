\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{booktabs}
\geometry{margin=1in}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\title{Mathematical Analysis of Manifold Intersection\\in Neural Network Weight Space}
\author{Experiment Results}
\date{January 2026}

\begin{document}
\maketitle

\section{Setup}

Let $\mathcal{W} = \mathbb{R}^{465}$ denote the parameter space of our neural network (a two-hidden-layer MLP with architecture $[10, 16, 16, 1]$).

For each dataset $i \in \{1, \ldots, 10\}$, we trained 50 models from random initializations near zero. Each model's final weights form a vector $w \in \mathcal{W}$.

For dataset $i$, we obtain a point cloud:
\[
W_i = \{w_i^{(1)}, w_i^{(2)}, \ldots, w_i^{(50)}\} \subset \mathbb{R}^{465}
\]

\textbf{Goal:} Determine whether the attractor manifolds for different datasets share a common ``universal'' subspace.

\textbf{Approximation:} We approximate each attractor manifold $M_i$ by the linear subspace $V_i$ spanned by the principal components of $W_i$.

\section{Method 1: PCA Ratio Analysis}

\subsection{Effective Dimension}

For dataset $i$, we compute the sample covariance matrix:
\[
\Sigma_i = \frac{1}{n-1}\sum_{j=1}^{n} (w_i^{(j)} - \bar{w}_i)(w_i^{(j)} - \bar{w}_i)^T
\]
where $\bar{w}_i = \frac{1}{n}\sum_j w_i^{(j)}$ is the sample mean and $n = 50$.

Let $\lambda_1^{(i)} \geq \lambda_2^{(i)} \geq \cdots \geq \lambda_{n-1}^{(i)} \geq 0$ be the eigenvalues of $\Sigma_i$.

\begin{definition}[Effective Dimension]
The \emph{effective dimension} (participation ratio) is:
\[
d_{\mathrm{eff}}^{(i)} = \frac{\left(\sum_k \lambda_k^{(i)}\right)^2}{\sum_k (\lambda_k^{(i)})^2} = \frac{(\mathrm{tr}\, \Sigma_i)^2}{\|\Sigma_i\|_F^2}
\]
\end{definition}

This quantity equals $m$ if all $m$ eigenvalues are equal, and equals 1 if one eigenvalue dominates.

\subsection{Results}

\begin{center}
\begin{tabular}{lc}
\toprule
Quantity & Value \\
\midrule
Individual $d_{\mathrm{eff}}^{(i)}$ (mean) & 41.5D \\
Individual $d_{\mathrm{eff}}^{(i)}$ (std) & 0.96D \\
Global $d_{\mathrm{eff}}^{\mathrm{global}}$ & 167.7D \\
Ratio & 3.79 \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Interpretation}

If all manifolds $M_i$ lie in a common subspace $S$ of dimension $d$:
\[
d_{\mathrm{eff}}^{\mathrm{global}} \approx d \approx d_{\mathrm{eff}}^{(i)} \quad \Rightarrow \quad \text{Ratio} \approx 1
\]

If manifolds are mutually orthogonal (span independent directions):
\[
d_{\mathrm{eff}}^{\mathrm{global}} \approx \sum_i d_{\mathrm{eff}}^{(i)} \quad \Rightarrow \quad \text{Ratio} \approx n_{\mathrm{datasets}}
\]

Our ratio of 3.79 indicates substantial independence between manifolds.

\section{Method 2: Direct Subspace Intersection}

\subsection{Setup}

For each dataset $i$, let $V_i \subset \mathbb{R}^{465}$ be the subspace spanned by the top $k_i$ principal components, where $k_i$ is chosen such that these components explain 95\% of the variance. In our experiment, $k_i \approx 44$ for all datasets.

We represent $V_i$ by a matrix $B_i \in \mathbb{R}^{465 \times k_i}$ whose columns form an orthonormal basis for $V_i$.

\subsection{Dimension Formula}

\begin{theorem}[Dimension of Intersection]
For subspaces $U, V$ of a vector space:
\[
\dim(U \cap V) = \dim(U) + \dim(V) - \dim(U + V)
\]
where $U + V = \{u + v : u \in U, v \in V\}$ is the sum of subspaces.
\end{theorem}

\begin{remark}
If $B_U$ and $B_V$ are matrices whose columns form bases for $U$ and $V$ respectively, then:
\[
\dim(U + V) = \mathrm{rank}([B_U \mid B_V])
\]
where $[B_U \mid B_V]$ denotes horizontal concatenation.
\end{remark}

\subsection{Computation}

For datasets 1 and 2:
\begin{align*}
\dim(V_1) &= 44 \\
\dim(V_2) &= 44 \\
\mathrm{rank}([B_1 \mid B_2]) &= 88
\end{align*}

Therefore:
\[
\dim(V_1 \cap V_2) = 44 + 44 - 88 = 0
\]

\subsection{Results}

We computed all $\binom{10}{2} = 45$ pairwise intersections:

\begin{center}
\begin{tabular}{lc}
\toprule
Statistic & Value \\
\midrule
Minimum pairwise intersection & 0D \\
Mean pairwise intersection & 0D \\
Maximum pairwise intersection & 0D \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Conclusion:} For all pairs $i \neq j$:
\[
V_i \cap V_j = \{0\}
\]

The subspaces are in \emph{general position}---they share only the origin.

\section{Method 3: Principal Angles}

\subsection{Definition}

\begin{definition}[Principal Angles]
For subspaces $U, V \subset \mathbb{R}^n$, the \emph{principal angles} $0 \leq \theta_1 \leq \theta_2 \leq \cdots \leq \theta_p \leq \frac{\pi}{2}$ (where $p = \min(\dim U, \dim V)$) are defined recursively by:
\[
\cos(\theta_k) = \max_{\substack{u \in U, \|u\| = 1 \\ v \in V, \|v\| = 1}} \langle u, v \rangle
\]
subject to $u \perp u_1, \ldots, u_{k-1}$ and $v \perp v_1, \ldots, v_{k-1}$.
\end{definition}

The vectors $u_k, v_k$ achieving the maximum are called \emph{principal vectors}.

\begin{remark}
\begin{itemize}
\item If $\theta_1 = 0$, then $U$ and $V$ share at least one direction.
\item $\dim(U \cap V) = \#\{k : \theta_k = 0\}$
\item If $\theta_1 > 0$, the subspaces intersect only at the origin.
\end{itemize}
\end{remark}

\subsection{Computation via Cosine Similarity}

For practical computation, we examined the cosine similarities between all pairs of basis vectors from different manifolds:
\[
s_{ij}^{(k,\ell)} = |\langle b_i^{(k)}, b_j^{(\ell)} \rangle|
\]
where $b_i^{(k)}$ is the $k$-th principal component of dataset $i$.

\subsection{Results}

\begin{center}
\begin{tabular}{lc}
\toprule
Statistic & Value \\
\midrule
Mean cross-manifold similarity & 0.042 \\
Maximum similarity & 0.298 \\
Pairs with similarity $> 0.9$ & 0 \\
\bottomrule
\end{tabular}
\end{center}

The maximum similarity of 0.298 corresponds to a minimum angle of:
\[
\theta_{\min} = \arccos(0.298) \approx 73Â°
\]

No two manifolds share any nearly-aligned directions.

\section{Method 4: MLE Intrinsic Dimension}

\subsection{Theory}

This nonlinear method (Levina \& Bickel, 2004) estimates intrinsic dimension from local neighborhood structure.

If data lies on a $d$-dimensional manifold with locally uniform density, the number of points within radius $r$ of a point $x$ scales as $r^d$.

\begin{definition}[MLE Dimension Estimator]
For a point $x$ with $k$-nearest neighbor distances $r_1 < r_2 < \cdots < r_k$:
\[
\hat{d}(x) = \left[\frac{1}{k-1} \sum_{j=1}^{k-1} \log \frac{r_k}{r_j}\right]^{-1}
\]
The global estimate is the average over all points.
\end{definition}

\subsection{Results}

\begin{center}
\begin{tabular}{lcc}
\toprule
Dataset Type & Individual MLE & Count \\
\midrule
Signal (synthetic 1--8) & $35.5$--$37.7$D & 8 \\
Noise (random labels) & $45.5$--$48.1$D & 2 \\
\midrule
Global (all 500 points) & 92.0D & --- \\
Mean individual & 38.4D & --- \\
Ratio & 2.40 & --- \\
\bottomrule
\end{tabular}
\end{center}

The ratio of 2.40 confirms that combined data spans significantly more dimensions than individual manifolds.

\section{Summary of Evidence}

\begin{center}
\begin{tabular}{lcc}
\toprule
Method & Intersection Estimate & Interpretation \\
\midrule
PCA Ratio & 0D & Ratio = 3.79 $\gg$ 1 \\
Subspace Intersection & 0D & All pairwise = 0 \\
Principal Angles & 0D & Max similarity = 0.30 \\
MLE & --- & Ratio = 2.40 $\gg$ 1 \\
\bottomrule
\end{tabular}
\end{center}

\section{Conclusion}

All four methods consistently indicate:

\begin{quote}
\textbf{The 10 datasets converge to separate, non-overlapping linear subspaces in weight space. There is no universal subspace shared across tasks.}
\end{quote}

Formally, if $V_i \subset \mathbb{R}^{465}$ denotes the (linear approximation to the) attractor manifold for dataset $i$, then:
\[
\bigcap_{i=1}^{10} V_i = \{0\}
\]

Moreover, the pairwise intersections vanish:
\[
\forall\, i \neq j: \quad V_i \cap V_j = \{0\}
\]

Each dataset has its own $\sim$41-dimensional attractor manifold, and these manifolds are in general position (nearly orthogonal).

\section{Caveats}

\begin{enumerate}
\item \textbf{Linear approximation:} We approximated each $M_i$ by a linear subspace via PCA. The true attractor could be a nonlinear manifold. Two nonlinear manifolds can intersect even if their tangent spaces at corresponding points do not.

\item \textbf{Finite sampling:} We used 50 points per manifold in $\mathbb{R}^{465}$. The effective dimension of $\sim$41D might be an artifact of undersampling (though saturation tests suggested dimension does stabilize).

\item \textbf{Affine vs.\ linear:} Each $V_i$ is centered at its own mean $\bar{w}_i$. The subspaces pass through different ``origins.'' We are comparing \emph{directions} of variation, not the actual point sets in $\mathbb{R}^{465}$.

\item \textbf{Dataset diversity:} The 10 datasets were generated from the same underlying distribution family (sklearn's \texttt{make\_classification}) with different random seeds. More diverse dataset types might reveal different structure.
\end{enumerate}

\end{document}
