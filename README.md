# Universal Subspace Hypothesis Investigation

An experimental framework to investigate whether trained neural network weights cluster around a low-dimensional manifold in weight space, and whether this manifold exhibits fractal-like properties.

## Background

The **Universal Subspace Hypothesis** suggests that well-trained neural networks of a given architecture may converge to weights that lie on or near a low-dimensional subspace or manifold within the high-dimensional parameter space. This project investigates:

1. Whether such a manifold exists across diverse tasks
2. What is the intrinsic dimensionality of this manifold
3. Whether the manifold has fractal-like structure (as suggested by the chaotic dynamics of backpropagation)

## Big Picture: How It Works

### The Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PHASE 1: TRAINING                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  11 Diverse Datasets  â†’  Small MLP (16-16)  â†’  Weight Vectors       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
â”‚  â€¢ Binary Classification     Input Layer          [wâ‚, wâ‚‚, ...]     â”‚
â”‚  â€¢ Multi-class                  â†“                      â†“             â”‚
â”‚  â€¢ Regression              Hidden (16)          Extract weights      â”‚
â”‚  â€¢ Time Series                  â†“                  as vectors        â”‚
â”‚                            Hidden (16)                â†“              â”‚
â”‚                                 â†“              (n_params dims)       â”‚
â”‚  [Train â†’ Converge]        Output Layer                              â”‚
â”‚  [Delete dataset]          [Save weights]      Weight Matrix         â”‚
â”‚  [Next dataset...]         [Delete model]      (11 Ã— n_params)      â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHASE 2: GEOMETRIC ANALYSIS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚   Weight Matrix (11 models Ã— ~600 params)                           â”‚
â”‚         â†“            â†“            â†“            â†“           â†“         â”‚
â”‚       â”Œâ”€â”€â”€â”        â”Œâ”€â”€â”€â”        â”Œâ”€â”€â”€â”       â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”       â”‚
â”‚       â”‚PCAâ”‚        â”‚MLEâ”‚        â”‚Boxâ”‚       â”‚UMAPâ”‚     â”‚DBSCANâ”‚     â”‚
â”‚       â””â”€â”¬â”€â”˜        â””â”€â”¬â”€â”˜        â””â”€â”¬â”€â”˜       â””â”€â”¬â”€â”˜      â””â”€â”¬â”€â”˜       â”‚
â”‚         â”‚            â”‚            â”‚           â”‚          â”‚           â”‚
â”‚    Effective    Intrinsic   Fractal Dim   Manifold   Clusters       â”‚
â”‚    Dimension    Dimension    (Box-count   Embedding  (Task types)   â”‚
â”‚    (95% var)    (k-NN MLE)   + Corr dim)  (2D, 3D)                  â”‚
â”‚         â”‚            â”‚            â”‚           â”‚          â”‚           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                              â†“                                        â”‚
â”‚                    Compare & Interpret:                              â”‚
â”‚                    â€¢ Is dim << n_params?  â†’ Universal subspace       â”‚
â”‚                    â€¢ Fractal â‰  Intrinsic? â†’ Fractal structure        â”‚
â”‚                    â€¢ Do tasks cluster?    â†’ Task similarity          â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PHASE 3: VISUALIZATION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  ğŸ“Š PCA Variance       ğŸ“ˆ Fractal Log-Log    ğŸ“Š Dimension Compare    â”‚
â”‚  ğŸ“Š UMAP 2D/3D         ğŸ“Š Clustering         ğŸ“Š Intrinsic Dim Dist   â”‚
â”‚                                                                       â”‚
â”‚  ğŸ“„ Summary Report: Conclusions about manifold structure             â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack by Component

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CORE PACKAGES & PURPOSES                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  ğŸ”¥ PyTorch (Model Definition & Training)                        â”‚
â”‚     â”œâ”€ torch.nn.Module         â†’ Neural network architecture     â”‚
â”‚     â”œâ”€ torch.nn.Linear         â†’ Layer definitions              â”‚
â”‚     â”œâ”€ torch.optim.Adam        â†’ Optimization                   â”‚
â”‚     â”œâ”€ torch.nn.*Loss          â†’ Loss functions                 â”‚
â”‚     â””â”€ torch.utils.data        â†’ Data loading & batching        â”‚
â”‚                                                                   â”‚
â”‚  ğŸ“Š NumPy (Numerical Computing)                                  â”‚
â”‚     â”œâ”€ Weight vector storage   â†’ np.ndarray                     â”‚
â”‚     â”œâ”€ Matrix operations       â†’ Linear algebra                 â”‚
â”‚     â””â”€ Distance calculations   â†’ pdist, norm                    â”‚
â”‚                                                                   â”‚
â”‚  ğŸ”¬ Scikit-learn (Classical ML & Data Processing)                â”‚
â”‚     â”œâ”€ StandardScaler          â†’ Data normalization             â”‚
â”‚     â”œâ”€ train_test_split        â†’ Data splitting                 â”‚
â”‚     â”œâ”€ PCA                     â†’ Dimensionality reduction        â”‚
â”‚     â”œâ”€ DBSCAN                  â†’ Density-based clustering        â”‚
â”‚     â”œâ”€ AgglomerativeClustering â†’ Hierarchical clustering        â”‚
â”‚     â”œâ”€ make_classification     â†’ Synthetic datasets             â”‚
â”‚     â”œâ”€ make_regression         â†’ Synthetic regression           â”‚
â”‚     â””â”€ load_* datasets         â†’ UCI datasets                   â”‚
â”‚                                                                   â”‚
â”‚  ğŸ—ºï¸ UMAP (Manifold Learning)                                     â”‚
â”‚     â””â”€ umap.UMAP               â†’ Non-linear embedding            â”‚
â”‚                                  (Better than t-SNE for this)   â”‚
â”‚                                                                   â”‚
â”‚  ğŸ“ SciPy (Scientific Computing)                                 â”‚
â”‚     â”œâ”€ pdist, squareform       â†’ Pairwise distances             â”‚
â”‚     â””â”€ linregress              â†’ Linear regression for fractal  â”‚
â”‚                                  dimension estimation            â”‚
â”‚                                                                   â”‚
â”‚  ğŸ“ˆ Matplotlib & Seaborn (Visualization)                         â”‚
â”‚     â”œâ”€ 2D/3D scatter plots     â†’ UMAP, PCA embeddings           â”‚
â”‚     â”œâ”€ Bar charts              â†’ Variance explained             â”‚
â”‚     â”œâ”€ Log-log plots           â†’ Fractal scaling                â”‚
â”‚     â””â”€ Histograms              â†’ Dimension distributions         â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Diagram

```
Dataset (sklearn/synthetic)
         â†“
    [NumPy arrays]
         â†“
  torch.FloatTensor  â†â”€â”€â”€â”€â”€â”
         â†“                  â”‚
  DataLoader (PyTorch)     â”‚
         â†“                  â”‚
  SmallMLP (PyTorch)       â”‚  TRAINING LOOP
         â†“                  â”‚  (PyTorch)
  Forward/Backward Pass    â”‚
         â†“                  â”‚
  Adam Optimizer  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
  Trained Model
         â†“
  model.parameters()  â†â”€â”€â”€ Extract weights
         â†“
  NumPy weight vector
         â†“
  [Stack all vectors]
         â†“
  Weight Matrix (NumPy)  â†â”€â”€â”€ 11 Ã— n_params
         â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
  â†“              â†“
sklearn.PCA    umap.UMAP    â†â”€â”€â”€ ANALYSIS
  â†“              â†“               (NumPy/SciPy)
Results       Results
  â†“              â†“
matplotlib.pyplot  â†â”€â”€â”€ VISUALIZATION
  â†“
PNG figures
```

### Module Responsibilities

| Module | Primary Package | Purpose |
|--------|----------------|---------|
| `models.py` | **PyTorch** | Define MLP architecture, extract weights |
| `datasets.py` | **scikit-learn**, PyTorch | Load/generate diverse datasets |
| `trainer.py` | **PyTorch**, NumPy | Train models, manage GPU/memory |
| `geometry_analysis.py` | **scikit-learn**, SciPy, **UMAP** | Compute dimensions, manifold properties |
| `visualization.py` | **Matplotlib**, Seaborn | Create plots and figures |
| `run_experiment.py` | All of above | Orchestrate full pipeline |

## Project Structure

```
hecke-algebra/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models.py           # Small neural network architectures
â”‚   â”œâ”€â”€ datasets.py         # Diverse dataset loaders (classification, regression, time series)
â”‚   â”œâ”€â”€ trainer.py          # Memory-efficient training loop
â”‚   â”œâ”€â”€ geometry_analysis.py # Geometric/topological analysis tools
â”‚   â””â”€â”€ visualization.py    # Plotting and visualization
â”œâ”€â”€ run_experiment.py       # Main experiment script
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ results/                # Output directory (created during run)
    â”œâ”€â”€ weight_matrix.npy
    â”œâ”€â”€ metadata.json
    â”œâ”€â”€ analysis_results.npz
    â”œâ”€â”€ summary_report.txt
    â””â”€â”€ figures/
```

## Installation

```bash
pip install -r requirements.txt
```

## Quick Start

Run the complete experiment with default settings:

```bash
python run_experiment.py
```

This will:
1. Train small MLPs (2 hidden layers, 16 neurons each) on 11 diverse datasets
2. Extract weight vectors from each trained model
3. Perform geometric analysis (PCA, fractal dimension, manifold learning, clustering)
4. Generate visualizations
5. Save results to `results/`

## Usage

### Basic Usage

```bash
# Run with default architecture (16-16 hidden layers)
python run_experiment.py

# Custom architecture (10-20-10 hidden layers)
python run_experiment.py --hidden-dims 10 20 10

# Faster training (fewer epochs)
python run_experiment.py --epochs 50 --patience 10

# Train on specific datasets only
python run_experiment.py --datasets binary_moons wine digits regression_synthetic
```

### Advanced Options

```bash
# Skip training and analyze existing results
python run_experiment.py --skip-training

# Only train, skip analysis (useful for collecting more data)
python run_experiment.py --skip-analysis

# Custom save directory
python run_experiment.py --save-dir my_experiment

# Full custom run
python run_experiment.py \
  --hidden-dims 20 20 \
  --lr 0.0005 \
  --epochs 150 \
  --patience 20 \
  --save-dir results_20x20
```

## Available Datasets

The experiment includes 11 diverse datasets across 4 task types:

**Binary Classification:**
- `binary_moons` - Two moons dataset
- `binary_circles` - Concentric circles
- `binary_classification_synthetic` - Synthetic 10D binary classification
- `breast_cancer` - Wisconsin breast cancer dataset

**Multi-class Classification:**
- `multi_classification_synthetic` - Synthetic 5-class problem
- `wine` - Wine quality dataset (3 classes)
- `digits` - Handwritten digits (10 classes)

**Regression:**
- `regression_synthetic` - Synthetic 10D regression
- `diabetes` - Diabetes progression prediction

**Time Series:**
- `time_series_sine` - Sine wave prediction
- `time_series_combined` - Combined sine/cosine waves

## Analysis Methods

### 1. PCA (Baseline)
Standard linear dimensionality reduction to establish a baseline for comparison.

### 2. Fractal Dimension Estimation
- **Box-counting method**: Measures how the number of boxes needed to cover the manifold scales with box size
- **Correlation dimension**: More robust estimate based on pairwise distances

### 3. Intrinsic Dimension (MLE)
Maximum likelihood estimation of the true dimensionality of the manifold based on local neighborhoods.

### 4. Manifold Learning (UMAP)
Non-linear dimensionality reduction that preserves both local and global structure, better than t-SNE for understanding manifold topology.

### 5. Clustering Analysis
DBSCAN clustering to identify if models naturally group by task type or other characteristics.

## Interpreting Results

After running the experiment, check `results/summary_report.txt` for key findings:

**Strong Universal Subspace Evidence:**
- PCA effective dimension < 10
- Fractal and intrinsic dimensions agree (within ~1)
- High clustering of models in low-dimensional space

**Fractal Structure Evidence:**
- Fractal dimension significantly different from intrinsic dimension
- Non-integer fractal dimension estimates
- Complex structure visible in UMAP embeddings

**Key Visualizations:**
- `pca_variance.png` - How much variance is captured by principal components
- `dimension_comparison.png` - Comparison of different dimensionality estimates
- `fractal_dimension.png` - Log-log plots showing fractal scaling
- `umap_2d.png` / `umap_3d.png` - Low-dimensional embeddings colored by task type
- `clustering.png` - Discovered clusters in weight space

## Research Questions

This framework helps investigate:

1. **Does a universal subspace exist?** â†’ Check PCA effective dimensionality
2. **Is it fractal?** â†’ Compare fractal vs intrinsic dimension estimates
3. **Do different tasks cluster?** â†’ Examine clustering results and UMAP embeddings
4. **How low-dimensional is it?** â†’ Compare all dimension estimates
5. **Is PCA sufficient?** â†’ Compare PCA vs UMAP embeddings

## Extending the Framework

### Add New Datasets

Edit `src/datasets.py`:

```python
@staticmethod
def _load_your_dataset(batch_size=32, **kwargs):
    # Your data loading code
    X, y = load_your_data()

    # Return train_loader, test_loader, metadata
    return train_loader, test_loader, metadata

# Add to ALL_DATASETS list
ALL_DATASETS.append('your_dataset')
```

### Modify Architecture

```bash
# Single hidden layer with 50 neurons
python run_experiment.py --hidden-dims 50

# Three hidden layers
python run_experiment.py --hidden-dims 32 16 8

# Larger network
python run_experiment.py --hidden-dims 64 64
```

### Add New Analysis Methods

Edit `src/geometry_analysis.py` to add methods to the `GeometricAnalyzer` class, then call them in `full_analysis()`.

## Technical Details

**Memory Management:**
- Datasets are loaded one at a time and deleted after training
- Models are deleted after weight extraction
- GPU cache is cleared after each training run

**Training:**
- Early stopping with patience (default: 15 epochs)
- Adam optimizer (default lr: 0.001)
- Task-appropriate loss functions (BCE, CrossEntropy, MSE)

**Computational Complexity:**
- Training: O(n_datasets Ã— n_epochs Ã— dataset_size)
- Geometric analysis: O(n_modelsÂ²) for pairwise distances
- UMAP: O(n_models Ã— log(n_models))

## Citation

If you use this framework in your research, please cite the relevant papers on the universal subspace hypothesis and fractal neural dynamics.

## License

MIT License